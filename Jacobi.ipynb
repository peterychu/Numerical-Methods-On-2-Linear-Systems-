{"cells": [{"cell_type": "markdown", "id": "2cc177d9-4cf5-4a9d-8307-6a2bf38ab429", "metadata": {}, "source": "Peter Chu, all code written between 8:05 PM March 6th to 1:20 AM March 7th"}, {"cell_type": "markdown", "id": "dd158508-f4b8-4695-b339-5d5a89c722af", "metadata": {}, "source": "## Jacobi Method"}, {"cell_type": "code", "execution_count": 1, "id": "00602372-d264-4a74-83a0-9b5068bf9042", "metadata": {}, "outputs": [], "source": "# import necessary libraries \nimport math as math\nimport numpy as np"}, {"cell_type": "markdown", "id": "18632fd5-391d-47ba-9b66-bd01dc1af2d0", "metadata": {}, "source": "### Part a"}, {"cell_type": "code", "execution_count": 2, "id": "54eebecc-a76a-46b6-98ea-61e71bad260f", "metadata": {}, "outputs": [], "source": "# Input A = matrix, b = sol vector, x = guess vector, tol = tolerance, n = max iterations, start writing code 8:05\n\ndef jacobi(A, b, x, tol, n):\n    \n    # Create necessary variables for formula\n    # D = diag matrix\n    # D_1 = inverse of D\n    # U = strictly upper matrix * -1 to make entries negative\n    # L = strictly lower matrix * -1 to make entries negative\n    # T = D^-1 * (L + U)\n    # count is a ticker variable to keep track of number of iterations\n    \n    D = np.diag(np.diag(A))\n    D_1 = np.linalg.inv(D)\n    U = (np.triu(ls1) - D).dot(-1)\n    L = (np.tril(ls1) - D).dot(-1)\n    T = D_1.dot(L + U)\n    count = 0\n    \n    # Do first iteration\n    x_k = x\n    x_k1 = (T.dot(x_k.T)) + (D_1.dot(b.T))\n    \n    count += 1\n    \n    # If x_0 = 0, ||X_0||_inf = 0 we will have a divide by zero error. \n    # Iterate one more time to avoid this and then enter loop\n    \n    if(x_k.all() == 0):\n        x_k = x_k1.T\n        x_k1 = (T.dot(x_k.T)) + (D_1.dot(b.T))\n        \n        count += 1\n    \n    # Create stop value for exit condition\n    StopVal = np.linalg.norm(abs(x_k1 - x_k), np.inf) / np.linalg.norm(x_k, np.inf)\n    \n    # Loop until we hit exit condition or we do n interations\n    while(StopVal > tol and count < n):\n        #Do jacobi method\n        x_k = x_k1.T\n        x_k1 = (T.dot(x_k.T)) + (D_1.dot(b.T))\n        \n        #Update stop value and add 1 to count\n        StopVal = np.linalg.norm(abs(x_k1 - x_k), np.inf) / np.linalg.norm(x_k, np.inf)\n        count += 1\n    \n    # Expect x^k vector and the number of iterations it took\n    \n    return x_k1, 'iterations:', count\n"}, {"cell_type": "markdown", "id": "3d11b9d9-0c43-494f-9766-793313141dad", "metadata": {}, "source": "### Part b"}, {"cell_type": "code", "execution_count": 3, "id": "1def04de-9f2f-456d-837f-ce4c729f339a", "metadata": {}, "outputs": [], "source": "# Use np.newaxis on vectors to make them transposable correctly\n# Python does not know how to transpose 1 dimensional arrays, need to make 2 dimensional to do so\n# ex. example = np.array([1,1,1])\n# example.T = np.array([1,1,1]) != np.array([1],[1],[1]) <- desired transposed vector \n\n# linear system 1\nls1 = np.array([[1,2,-2], [1,1,1], [2,2,1]])\nb1 = np.array([7,2,5])[np.newaxis]\n\n# Linear system 2\n\nls2 = np.array([[2,-1,1], [2,2,2], [-1,-1,2]])\nb2 = np.array([-1,4,-5])[np.newaxis]\n\n# initial guess\nx_0 = np.array([0,0,0])[np.newaxis]\n\n# tolerance\ntolVal = 0.00005 "}, {"cell_type": "code", "execution_count": 4, "id": "51c91715-1194-4571-8ef3-f0e6d61830af", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "(array([[ 1.],\n       [ 2.],\n       [-1.]]), 'iterations:', 25) \n (array([[-8262.5],\n       [ 7400. ],\n       [ 6387.5]]), 'iterations:', 25)\n"}], "source": "# Run Jacobi function using defined variables\n\naprox1 = jacobi(ls1, b1, x_0, tolVal, 25)\n\naprox2 = jacobi(ls2, b2, x_0, tolVal, 25)\n\nprint(aprox1, '\\n', aprox2)"}, {"cell_type": "markdown", "id": "d213b951-8e3c-4223-ab98-6e2e039acc48", "metadata": {}, "source": "### Part c"}, {"cell_type": "code", "execution_count": 5, "id": "6b214468-dc8e-4b04-94c5-6ecadd3bcdb9", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Spectral radius of linear system 1: 1.0813325779705841e-05 \n\nSpectral radius of linear system 2: 1.1180339887498956\n"}], "source": "# Calculate spectral radius for ls1 and store value as lambda1 \n\nD = np.diag(np.diag(ls1))\nU = (np.triu(ls1) - D).dot(-1)\nL = (np.tril(ls1) - D).dot(-1)\nD_1 = np.linalg.inv(D)\n\nT = D_1.dot(L + U)\n\nlambda1 = max(abs(np.linalg.eig(T)[0]))\n\n# Calculate spectral radius for ls2 and store value as lambda2\n\nD = np.diag(np.diag(ls2))\nU = (np.triu(ls2) - D).dot(-1)\nL = (np.tril(ls2) - D).dot(-1)\nD_1 = np.linalg.inv(D)\n\nT = D_1.dot(L + U)\n\nlambda2 = max(abs(np.linalg.eig(T)[0]))\n\nprint('Spectral radius of linear system 1:', lambda1, '\\n')\nprint('Spectral radius of linear system 2:', lambda2)"}, {"cell_type": "markdown", "id": "a32a3ac5-9a17-41fa-94d2-ac359831a65f", "metadata": {}, "source": "The Jacobi method gave a good approximation for linear system 1. Looking at $\\rho(T_j)$ for linear system 1, we have $\\rho(T_g) < 1$. Thus $T_j$ is convergent and converges to certain values. As a result we get a good approximation because $x^k$ converges as well. For linear system 2, we have that $\\rho(T_j) > 1$ which results in $T_j$ being divergent and thus $x^k$ does not converge to some value. This results in a bad approximation. "}, {"cell_type": "markdown", "id": "597fa2e1-4808-47a2-a324-d57f38afd09a", "metadata": {}, "source": "## Gauss-Seidel Method"}, {"cell_type": "markdown", "id": "0193e14f-00ca-4020-8cc3-8ed3f3fe4b5f", "metadata": {}, "source": "### Part a"}, {"cell_type": "code", "execution_count": 6, "id": "2c40a5a7-61e4-4426-9f38-07b0292854c3", "metadata": {}, "outputs": [], "source": "# Input A = matrix, b = sol vector, x = guess vector, tol = tolerance, n = max iterations\n\ndef GaussSeidel(A, b, x, tol, n):\n    \n    # Create necessary variables for formula\n    # D = diag matrix\n    # U = strictly upper matrix * -1 to make entries negative\n    # L = strictly lower matrix * -1 to make entries negative\n    # DL_1 = (D - L)^-1, T = (D - l)^-1 * U\n    # count is a ticker variable to keep track of number of iterations\n    \n    D = np.diag(np.diag(A)) \n    U = (np.triu(A) - D).dot(-1)\n    L = (np.tril(A) - D).dot(-1)\n    DL_1 = np.linalg.inv(D - L)\n    T = DL_1.dot(U)\n    count = 0\n    \n    # Do first iteration\n    \n    x_k = x\n    x_k1 = T.dot(x_k.T) + DL_1.dot(b.T)\n    \n    # Update number of iterations\n    count += 1\n    \n    #If x_0 = 0, ||X_0||_inf = 0 we will have a divide by zero error. \n    #Iterate one more time to avoid this and then enter loop\n    \n    if(x_k.all() == 0):\n        x_k = x_k1.T\n        x_k1 = T.dot(x_k.T) + DL_1.dot(b.T)\n        count += 1\n    \n    # Create stop value for exit condition\n    StopVal = np.linalg.norm(abs(x_k1 - x_k), np.inf) / np.linalg.norm(x_k, np.inf)\n    \n    # Loop until we hit exit condition or we do n interations\n    while(StopVal > tol and count < n):\n        #Do Gauss-Seidel method\n        x_k = x_k1.T\n        x_k1 = T.dot(x_k.T) + DL_1.dot(b.T)\n        \n        #Update stop value and add 1 to count\n        StopVal = np.linalg.norm(abs(x_k1 - x_k), np.inf) / np.linalg.norm(x_k, np.inf)\n        count += 1\n    \n    # Expect x^k vector and the number of iterations it took\n    \n    return x_k1, 'iterations:',  count\n    "}, {"cell_type": "markdown", "id": "00a97252-0633-4071-8404-9486a1501c87", "metadata": {}, "source": "### Part b"}, {"cell_type": "code", "execution_count": 7, "id": "0645e5e2-4a9a-4a4c-9a9e-6fd4f75a6057", "metadata": {}, "outputs": [], "source": "# Use np.newaxis on vectors to make them transposable correctly\n# Python does not know how to transpose 1 dimensional arrays, need to make 2 dimensional to do so\n# ex. example = np.array([1,1,1])\n# example.T = np.array([1,1,1]) != np.array([1],[1],[1]) <- desired transposed vector \n\n# linear system 1\n\nls1 = np.array([[1,2,-2], [1,1,1], [2,2,1]])\nb1 = np.array([7,2,5])[np.newaxis]\n\n# Linear system 2\n\nls2 = np.array([[2,-1,1], [2,2,2], [-1,-1,2]])\nb2 = np.array([-1,4,-5])[np.newaxis]\n\n# initial guess\nx_0 = np.array([0,0,0])[np.newaxis]\n\n# tolerance\ntolVal = 0.00005"}, {"cell_type": "code", "execution_count": 8, "id": "66044ea4-a255-4a6f-b92c-ac4a506080ab", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "(array([[ 1.30862285e+09],\n       [-1.32540006e+09],\n       [ 3.35544310e+07]]), 'iterations:', 25) \n (array([[ 1.00000063],\n       [ 1.99999931],\n       [-1.00000003]]), 'iterations:', 25)\n"}], "source": "# Run Gauss Seidel using defined variables\n\naprox1 = GaussSeidel(ls1, b1, x_0, tolVal, 25)\n\naprox2 = GaussSeidel(ls2, b2, x_0, tolVal, 25)\n\nprint(aprox1, '\\n', aprox2)"}, {"cell_type": "markdown", "id": "dc85af6e-98a4-4563-9ec2-8dd05c627d50", "metadata": {}, "source": "### Part c"}, {"cell_type": "code", "execution_count": 9, "id": "b0e5a501-f140-4a10-b65b-f674592d34e6", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Spectral radius of linear system 1: 2.0 \n\nSpectral radius of linear system 2: 0.5\n"}], "source": "# Calculate spectral radius for ls1 and store value as lambda1, finish code 1:20\n\nD = np.diag(np.diag(ls1)) \nU = (np.triu(ls1) - D).dot(-1)\nL = (np.tril(ls1) - D).dot(-1)\nDL_1 = np.linalg.inv(D - L)\nT = DL_1.dot(U)\n\nlambda1 = max(abs(np.linalg.eig(T)[0]))\n\n# Calculate spectral radius for ls2 and store value as lambda2\n\nD = np.diag(np.diag(ls2)) \nU = (np.triu(ls2) - D).dot(-1)\nL = (np.tril(ls2) - D).dot(-1)\nDL_1 = np.linalg.inv(D - L)\nT = DL_1.dot(U)\n\nlambda2 = max(abs(np.linalg.eig(T)[0]))\n\nprint('Spectral radius of linear system 1:', lambda1, '\\n')\nprint('Spectral radius of linear system 2:', lambda2)"}, {"cell_type": "markdown", "id": "a3f22890-4ec6-473a-b1ea-bf85a37d2069", "metadata": {"tags": []}, "source": "The Gauss-Seidel method gave a good approximation for linear system 2. Looking at $\\rho(T_g)$ for linear system 2, we have $\\rho(T_g) = 0.5 < 1$. Thus $T_g$ is convergent and converges to certain values. As a result we get a good approximation because $x^k$ converges as well. For linear system 1, we have that $\\rho(T_j) = 2 > 1$ which results in $T_j$ being divergent and thus $x^k$ does not converge to some value. This results in a bad approximation. "}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}